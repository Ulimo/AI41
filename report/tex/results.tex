\section{Results and analysis}
This chapter shows the different results from the experiments. Both on how the different smoothing techniques, grammar constraints and context changed the number of keystrokes needed to complete a sentence, and a qualitative analysis of the results from using context recognition.
\input{./tex/results_tables.tex}
\subsection{Keystrokes analysis}
Tables 1, 2 and 3 with case sensitive N-grams show that absolute discounting present better predictions than Kneser-Ney in two of the sentences. This differs slightly from our hypothesis that Kneser-Ney should present better predictions. The case insensitive tables, table 4, 5 and 6, show that the difference is smaller. Both smoothing techniques require the same amount of keystrokes for the first sentence. The difference between case sensitive and insensitive N-grams can be that Kneser-Ney uses a continuation count to calculate its probabilities. In the case sensitive case “The car” and “the car” becomes two different N-grams and the continuation count of “the” will therefore be wrong in that case. This could explain the result from the case sensitive result, since the continuation count gets distorted.

Using grammar constraints produced worse results compared to having no grammar constraints. This can be due to many reasons, e.g. badly implemented or conflicting grammar constraints. Since the English language is rather complex there are few rules which can be applied for all situations. Aurora-systems which developed a word predictor writes that grammatical rules only apply if the writer does not know how to formulate correct sentences, otherwise it is more of a hindrance\cite{aurora}.

The context recognition results show that the fabricated sentence, as explained in test sentences, performs better than without context recognition. This is because the sentence is fabricated and the corpus contains the N-gram “the world was wide”. In the first sentence the context recognition does not provide any improvements. This shows that for the context recognition to work, there actually needs to be an N-gram which contains the actual sentence. In regards to the hypothesis this is quite correct, as long as there are N-grams which contain the contextual word there will be a better prediction. This could mean that as found by Gregory W. Lesher et.al that the performance scales with the training set size\cite{Lesher99effectsof}, this implementation of context recognition may scale depending on training set size as well.
\vspace{1em}
\subsection{Analysing predictions with context recognition}
Table 7 shows that the two results are identical and solely rely on the N-gram “it was”. In comparison, table 8 shows that when “it” has been replaced with the first appearing noun, the predicted words are more specific and linked to the context.

Table 9 shows the prediction with maximum-likelihood smoothing, and the suggested words can be seen to not be as general as compared to Kneser-Ney. This is visible in the keystrokes result as well, table 4 and table 6, where ML needed three more keystrokes to complete the sentence.