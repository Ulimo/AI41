\section{Results and analysis}
This chapter shows the different results from the experiments. Both on how the different smoothing techniques, grammar constraints and context changed the number of keystrokes needed to complete a sentence, and a qualitative analysis of the results from using context recognition.
\input{./tex/results_tables.tex}
\subsection{Keystrokes analysis}
Tables 1, 2 and 3 with case sensitive N-grams show that absolute discounting gives better predictions than Kneser-Ney in two of the sentences. This differs slightly from our hypothesis that Kneser-Ney should present better predictions. The case insensitive tables, table 4, 5 and 6, show that the difference is smaller. Both smoothing techniques require the same amount of keystrokes for the first sentence. The difference between case sensitive and insensitive N-grams can be that Kneser-Ney uses a continuation count to calculate its probabilities. In the case sensitive case “The car” and “the car” make two different N-grams and the continuation count of “the” will therefore be wrong in that case. This could explain the result from the case sensitive result, since the continuation count gets distorted.

Using grammar constraints produced worse results compared to having no grammar constraints. This could be due to many reasons, e.g. badly implemented or conflicting grammar constraints. Since the English language is rather complex there are few rules which can be applied for all situations. Aurora-systems which developed a word predictor writes that grammatical rules only apply if the writer does not know how to formulate correct sentences, otherwise it is more of a hindrance\cite{aurora}.

The context recognition results showed that the fabricated sentence, as explained in REFtest sentences, performed better with context recognition than without. This was probably because the sentence was fabricated and the corpus contains the N-gram “the world was wide”. In the first sentence the context recognition did not provide any improvements. This showed that for the context recognition to work, there actually needs to be a matching N-gram. This was in line with the hypothesis, as long as the corpus was relevant for the sentence. Gregory W. Lesher et.al stated that the performance of word predictors scales with the training set size\cite{Lesher99effectsof}, which seemed to be in line with the implementation of context recognition.
\vspace{1em}
\subsection{Analysing predictions with context recognition}
Table 7 shows that the two results were identical and solely rely on the N-gram “it was”. In comparison, table 8 shows that when “it” had been replaced with the first appearing noun, the predicted words were more specific and linked to the context.

Table 9 shows the prediction with maximum-likelihood smoothing, and the suggested words could be seen to be less general as compared to Kneser-Ney. This was visible in the keystrokes result as well: table 4 and table 6, where ML needed three more keystrokes to complete the sentence.