\section{Introduction}

Word predictors are programs that can estimate the sequence or part of the sequence of words for a given text sentences.Traditionally these kind of programs have been used in the the context of health care in aiding people with disabilities to communicate \cite{Lesher99effectsof}. During the last decades these programs have been a part in the field of augmentative communication but this time directed towards the border mass. It have become a tool for increasing efficiency of typing in general \cite{Lesher99effectsof}. 

There are several ways of making word predictors. Most of the word prediction make use of N-gram language models to estimate the probability of the following word in a phrase \cite{Wandmacher}. N-grams are a way to group and sample words that occur next to each other in text. It grasps the probability of sequences of words rather than the single word alone. I the sampling of word sequences also captures part of the grammar in a language due to it keeps the order of the words within the sequences. The N i N-grams stand for the number of words in the word sequence sampled.

There are several methods to even further refine the nature of natural language. 
One way of the possible ways is to implement grammar constraints that take in account that words can/can not follow in any arbitrary order. In this way it’s possible to limit the number of possible words that might follow the most recent word predicted solly by probability but are not correct according to the semantics.

Yet another way might also be to take in account the context of the text. By identifying different parts of the sentence construction, e.g. the acknowledgement of earlier written subjects and objects in main respectively dependent clauses, the next word can be predicted further. This could also add to the constraints of words possible due to need of matching with the already written words. 
 
How to make word predictors more efficient is the main question of this project and it’s going to be looked into by the means of experiments. 

\subsection{Problem statement/formulation}
Word prediction is an important part in Natural Language Processing (NLP) since in many tasks it is necessary to determine the next word  \cite{DBLP:journals/corr/cs-CL-0009027}.  Predicting words correctly can be a difficult task since there are a lot of factors that have to be taken into account. Grammar constraints and context of the input are often complex matters which need to be analyzed in order to make accurate predictions.
 
Gregory W. Lesher et.al \cite{Lesher99effectsof} show that the performance has been seen to rely on training text size. It is mentioned in the report that the performance of a word predictor could be improved with syntax-based prediction which should be investigated. This is also mentioned by Keith Trnka et.al.\cite{keystrokes} that syntactic knowledge can help improve word prediction.
 
One goal of word predictors is to be able to give accurate results to sentences in different contexts \cite{DBLP:journals/corr/cs-CL-0009027}. This requires a way for the word predictor to recognize the context in the sentence.
 
Both syntax-based prediction (grammatical analysis) and context recognition require to be implemented along with a model. This work uses N-gram modelling since it is a common tool in NLP \cite{Wandmacher}. A problem with N-gram moddeling is that it uses training data which makes it possible that not all combinations of words occur, which requires a smoothing technique to give unseen N-grams some probability \cite{Russel}. Smoothing techniques are a requirement for word prediction to be usable for different sentences than those in the training data.
 
Combining  these techniques together: smoothing, grammatical rules and context recognition, is the problem this report focuses on, and how they can improve a word predictors' performance.

\subsection{Hypothesis}
In this section the hypothesis for this work is explained in the different categories: Smoothing techniques, grammatical constraints and context recognition.
\subsubsection{Smoothing techniques}
The smoothing techniques evaluated are kneser-ney smoothing and absolute discount smoothing. Both these smoothing techniques rely on similar ideas. They use a constant discount value to distribute probability from known N-grams to unknown N-grams. This idea can be seen from Good-Turing smoothing where the discounted value follow a similar pattern with a near constant discount \cite{coursera}.

Both the smoothing techniques interpolate their value from their lower N-grams but with different approaches. Absolute discounting uses the probability of the unigrams to interpolate the probability of a word, while Kneser-ney instead uses the continuation count of the word \cite{coursera}.

Our hypothesis regarding how the different smoothing techniques will perform in comparison to each other is that for a well-balanced corpus, they may provide similar predictions to each other. For instance for predicting the word after “it was”, the most popular words that follow this sentence can already be used in many different contexts and have a high unigram probability. If on the other hand the corpus is unbalanced and there is an unusual word which is overrepresented, it may have a high unigram probability. In absolute discounting this will give the unusual word a high prediction as an unseen word. For kneser-ney on the other hand it will still pick the words that are usable in many different contexts.

So for a balanced corpus, they may be quite similar, while with an unbalanced corpus Kneser-ney will provide better results.
\subsubsection{Grammatical constraints}
Grammatical constraints tries to apply the grammatical rules in a language to help the predictor to get more reasonable results. Our hypothesis is that these constraints will help in many cases to remove words that is unusable in a certain sentence. Since they can look on the complete sentence regardless of the N-gram sizes, they may provide valuable information on which N-grams that are usable in that current sentence. So the result using grammatical constraints should be better than without it.
\subsubsection{Context recognition}
By context recognition, it is meant that the word predictor tries to understand the context that a sentence is about and will try to provide suggestions based on that context. Context recognition may be both good and bad. If the context is misinterpreted the suggestions may be worse than without context recognition. Example of sentences which can be hard to understand the context in:

\begin{itemize}
\item My dog and I went to the beach and it was happy
\item My dog and I went to the beach and it was nice
\end{itemize}
These sentences are almost exactly the same but talk about different things. The first sentence talk about the dog, while the second sentence more talk about the experience.
Since the English language is a Subject-verb-object language, our hypothesis is that the probability to get the correct context should be higher than misinterpreting it.

\subsection{Contribution}
The results of this work could guide anyone who wants to implement a good word predictor. It may also be used to draw conclusions regarding questions that are not the main focus of this work. An example of such questions are ''when does a word predictor have to be efficient?''. Functionality that is bad in theory because it should perform very badly for some situations may prove practically good in a statistical research. Another question is ''what functionality is worth implementing?''. Some functionality may provide minimal improvement while being extremely resource hungry. Hopefully, this report will contribute to the field by providing results and discussions that could help give answers such questions.

\subsection{Outline}