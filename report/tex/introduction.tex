\section{Introduction}
Word predictors have traditionally been used to increase the communication ability of persons with speech and language impairments\cite{Lesher99effectsof}\cite{keystrokes}\cite{corpus}. More recently, they have also come to use in increasing efficiency of typing in general \cite{Lesher99effectsof}, such as in-line search suggestions in search engines and the \emph{prediction window} for touch screen keyboards. 

\paragraph{Problem statement}
Presenting word suggestions to a user requires a statistical language model\cite{keystrokes} to calculate the probability $P(word|history)$, the most common being N-gram models\cite{Wandmacher}. The problem and the goal of this work is to find what makes such a word predictor effective. In this work, by effective, it is meant that a more effective word predictor requires a lower KSR (Key Stroke Rate, calculated as $KSR = \frac{\text{keystrokes made}}{\text{keystrokes in sentence}}$) than a less effective one. More specifically, the goal is to test different functionalities that are thought to increase the effectiveness of a word predictor and evaluate the results. The problem is worth addressing because the solution could help making communication with machines more effective in general. Word prediction could also be used as an important component in other areas of NLP, such as speech recognition, why their effectiveness is a crucial part in NLP.

\paragraph{Hypotheses}
Three different functionalities that were thought to affect the effectiveness of word predictors. For each of these, a hypothesis was come up with to predict how the function would affect effectiveness.

The first function, probability smoothing, is used to assign a non-zero probability to events unseen in the training data\cite{smoothing}. N-gram models as such require smoothing to give unseen N-grams probability\cite{Russel}. The smoothing techniques tested in this work are \emph{Kneser-Ney} and \emph{absolute discounting}. They both use a constant discount value to distribute probability from known to unknown N-grams, an idea that was derived from \emph{Good-Turing smoothing} where discounted values follow a similar pattern with a near constant discount\cite{coursera}. The main difference between the two is that absolute discounting uses unigrams to interpolate the probability of a word; Kneser-Ney uses continuation count, that is the number of different contexts in which a given word appears in the training data\cite{coursera}. The hypothesis regarding these smoothing techniques is that words with high unigram probability are likely to be overrepresented using \emph{absolute discounting}, while \emph{Kneser-Ney} should give a more varied result.

The second function is grammatical constraints, which means using grammatic rules to get more reasonable results. By excluding words that are known to not be the correct words to guess, the probability of predicting the right word is increased. The second hypothesis is therefore that grammar constraints will decrease the likelihood of predicting impossible words.

The last function, context recognition, involves recognizing the topic of the sentence with the objective to predict more relevant words. Previous studies have shown that topic modeling can be used to improve prediction\cite{DBLP:journals/corr/cs-CL-0009027}\cite{keystrokes}. This work makes the hypothesis that context recognition will help word predictors predicting more relevant words, increasing the likelihood of predicting the word the user meant to write.

\subsection{Contribution}
The results of this work could guide anyone who wants to implement a good word predictor. It may also be used to draw conclusions regarding questions that are not the main focus of this work. An example of such questions are ''when does a word predictor have to be efficient?''. Functionality that is bad in theory because it should perform very badly for some situations may prove practically good in a statistical research. Another question is ''what functionality is worth implementing?''. Some functionality may provide minimal improvement while being extremely resource hungry. Hopefully, this report will contribute to the field by providing results and discussions that could help give answers such questions.
