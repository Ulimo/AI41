\section{Summary}
The goal of this work was to find what functionality could help making word predictors effective. In striving to achieve this, an implementation which used common techniques for improving performance was made. KYLM and OpenNLP that were used to generate N-gram probabilities and POS-tagging respectively, were the foundational stones for this implementation. On top of that, development of grammar constraints and context recognition was made to make possible experiments to evaluate what could make a word predictor more effective. The results of the tests in this work showed that grammar constrains did not seem to benefit the effectiveness of the word predictor as expected, while context recognition seemed to do so, given that the corpus contained usable N-grams, which was in line with the expectations.

Grammar constraints proved hard to both implement and to test, and more work needs to be done to establish their effects on word predictor effectiveness. It is likely that quantitative methods are more effective for testing grammar constraints since the number of positive and negative cases ought to be numerous.

\section{Contribution}
We, the members of group 41, unanimously declare that we have all equally contributed toward the completion of this project.
